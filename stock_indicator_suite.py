#!/usr/bin/env python3
"""
üî¨üî¨üî¨ ULTIMATE STOCK INDICATOR SUITE & CEO-MATCH DIAGNOSTICS üî¨üî¨üî¨

50+ stock indicators across 8 categories with comprehensive CEO match analysis:

INDICATOR CATEGORIES:
1. Momentum (6): 1M, 3M, 6M, 12M momentum, acceleration
2. Reversal (5): Long-term reversal, mean reversion, 52-week distance
3. Volatility (8): Total, idiosyncratic, downside, GARCH persistence
4. Liquidity (5): Turnover, Amihud illiquidity, zero-day %
5. Factor Loadings (8): CAPM beta, up/down beta, FF3, momentum factor
6. Risk-Adjusted (8): Alpha, Sharpe, Sortino, Calmar, Information ratio
7. Tail Risk (9): VaR, CVaR, max drawdown, skewness, kurtosis
8. Persistence (7): Autocorrelation, volatility clustering, recovery

DIAGNOSTIC SUITE:
A. Stratification Analysis (all indicators by match quintile)
B. Regression Analysis (univariate, multivariate, fixed effects)
C. Machine Learning (Random Forest, SHAP)
D. Interaction Effects (Match √ó Size, Match √ó Industry)
E. Persistence Analysis (Match ‚Üí Future indicator stability)
F. Causal Attempts (Event studies around CEO changes)
"""
import pandas as pd
import numpy as np
import wrds
import torch
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from torch.utils.data import DataLoader
import seaborn as sns
from scipy import stats
from statsmodels.formula.api import ols
import warnings
warnings.filterwarnings('ignore')
import os
from datetime import datetime

from ceo_firm_matching import (
    Config,
    DataProcessor,
    CEOFirmDataset,
    train_model,
)

plt.style.use('seaborn-v0_8-whitegrid')
np.random.seed(42)

# WRDS Credentials
WRDS_USER = 'maricste93'
WRDS_PASS = 'jexvar-manryn-6Cosky'

# ================================================================
# CONFIGURATION
# ================================================================
class IndicatorConfig:
    """Configuration for stock indicator suite."""
    
    # Momentum windows
    MOMENTUM_WINDOWS = [1, 3, 6, 12]
    
    # Reversal windows
    REVERSAL_WINDOWS = [36, 60]
    
    # Rolling window for factor models
    FACTOR_WINDOW = 36  # months
    
    # VaR confidence levels
    VAR_LEVELS = [0.95, 0.99]
    
    # Output directory
    OUTPUT_DIR = 'Output'
    
    # Fama-French factors URL (backup)
    FF_URL = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip'


# ================================================================
# WRDS CONNECTION
# ================================================================
def connect_wrds():
    """Connect to WRDS."""
    try:
        os.environ['PGPASSWORD'] = WRDS_PASS
        db = wrds.Connection(wrds_username=WRDS_USER)
        print("‚úÖ WRDS Connection Successful!")
        return db
    except Exception as e:
        print(f"‚ùå WRDS Connection Failed: {e}")
        return None


# ================================================================
# DATA PULL FUNCTIONS
# ================================================================
def pull_crsp_monthly(db, start_year=2006):
    """Pull comprehensive CRSP monthly data."""
    print("\nüìä Pulling CRSP Monthly Data...")
    query = f"""
    SELECT a.permno, a.date, 
           a.ret, a.retx, a.prc, a.vol, a.shrout,
           a.cfacpr, a.cfacshr,
           ABS(a.prc) as abs_prc
    FROM crsp.msf a
    WHERE a.date >= '{start_year}-01-01'
      AND a.ret IS NOT NULL
      AND a.prc IS NOT NULL
    ORDER BY a.permno, a.date
    """
    try:
        df = db.raw_sql(query)
        df['date'] = pd.to_datetime(df['date'])
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['yearmonth'] = df['year'] * 100 + df['month']
        
        # Market cap
        df['mktcap'] = np.abs(df['prc']) * df['shrout']
        
        # Turnover
        df['turnover'] = df['vol'] / df['shrout'].replace(0, np.nan)
        
        # Dollar volume
        df['dollar_volume'] = df['vol'] * np.abs(df['prc'])
        
        print(f"   Retrieved {len(df):,} stock-month observations")
        return df
    except Exception as e:
        print(f"   Error: {e}")
        return None


def pull_crsp_daily(db, start_year=2006):
    """Pull CRSP daily data for high-frequency metrics."""
    print("\nüìä Pulling CRSP Daily Data (for volatility estimation)...")
    query = f"""
    SELECT a.permno, 
           EXTRACT(YEAR FROM a.date) as year,
           EXTRACT(MONTH FROM a.date) as month,
           COUNT(*) as n_days,
           STDDEV(a.ret) as daily_vol,
           SUM(CASE WHEN a.ret = 0 THEN 1 ELSE 0 END) as zero_days,
           MIN(a.ret) as min_ret_daily,
           MAX(a.ret) as max_ret_daily,
           SUM(ABS(a.ret) / NULLIF(ABS(a.prc) * a.vol, 0)) as amihud_sum,
           AVG(ABS(a.askhi - a.bidlo) / NULLIF((a.askhi + a.bidlo)/2, 0)) as spread_proxy
    FROM crsp.dsf a
    WHERE a.date >= '{start_year}-01-01'
      AND a.ret IS NOT NULL
    GROUP BY a.permno, EXTRACT(YEAR FROM a.date), EXTRACT(MONTH FROM a.date)
    """
    try:
        df = db.raw_sql(query)
        df['yearmonth'] = df['year'].astype(int) * 100 + df['month'].astype(int)
        
        # Realized volatility (annualized from daily)
        df['realized_vol'] = df['daily_vol'] * np.sqrt(252)
        
        # Zero-day percentage
        df['zero_day_pct'] = df['zero_days'] / df['n_days']
        
        # Amihud illiquidity (average)
        df['amihud'] = df['amihud_sum'] / df['n_days']
        
        print(f"   Retrieved {len(df):,} stock-month observations")
        return df
    except Exception as e:
        print(f"   Error: {e}")
        return None


def pull_ff_factors(db, start_year=2006):
    """Pull Fama-French factors from WRDS."""
    print("\nüìä Pulling Fama-French Factors...")
    query = f"""
    SELECT date, mktrf, smb, hml, rf, umd
    FROM ff.factors_monthly
    WHERE date >= '{start_year}-01-01'
    ORDER BY date
    """
    try:
        df = db.raw_sql(query)
        df['date'] = pd.to_datetime(df['date'])
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['yearmonth'] = df['year'] * 100 + df['month']
        print(f"   Retrieved {len(df):,} months of factors")
        return df
    except Exception as e:
        print(f"   Error pulling from WRDS, trying backup: {e}")
        return None


def pull_link_table(db):
    """Pull CRSP-Compustat link table."""
    print("\nüìä Pulling Link Tables...")
    query = """
    SELECT DISTINCT gvkey, lpermno as permno, linkdt, linkenddt
    FROM crsp.ccmxpf_linktable
    WHERE linktype IN ('LU', 'LC')
      AND linkprim IN ('P', 'C')
    """
    try:
        df = db.raw_sql(query)
        df['gvkey'] = pd.to_numeric(df['gvkey'], errors='coerce')
        print(f"   Retrieved {len(df):,} links")
        return df
    except Exception as e:
        print(f"   Error: {e}")
        return None


# ================================================================
# INDICATOR CONSTRUCTION FUNCTIONS
# ================================================================
class IndicatorBuilder:
    """Build all stock indicators from CRSP data."""
    
    def __init__(self, crsp_monthly, crsp_daily, ff_factors):
        self.monthly = crsp_monthly.copy()
        self.daily = crsp_daily.copy() if crsp_daily is not None else None
        self.ff = ff_factors.copy() if ff_factors is not None else None
        
        # Sort for rolling calculations
        self.monthly = self.monthly.sort_values(['permno', 'date'])
        
    def build_all(self):
        """Build all indicators."""
        print("\n" + "=" * 70)
        print("üîß BUILDING 50+ STOCK INDICATORS")
        print("=" * 70)
        
        df = self.monthly.copy()
        
        # Category 1: Momentum
        print("\nüìà Category 1: Momentum Indicators...")
        df = self._build_momentum(df)
        
        # Category 2: Reversal
        print("üìâ Category 2: Reversal Indicators...")
        df = self._build_reversal(df)
        
        # Category 3: Volatility
        print("üìä Category 3: Volatility Indicators...")
        df = self._build_volatility(df)
        
        # Category 4: Liquidity
        print("üíß Category 4: Liquidity Indicators...")
        df = self._build_liquidity(df)
        
        # Category 5: Factor Loadings
        print("‚ö° Category 5: Factor Loadings...")
        df = self._build_factor_loadings(df)
        
        # Category 6: Risk-Adjusted
        print("‚öñÔ∏è Category 6: Risk-Adjusted Metrics...")
        df = self._build_risk_adjusted(df)
        
        # Category 7: Tail Risk
        print("üé≤ Category 7: Tail Risk Indicators...")
        df = self._build_tail_risk(df)
        
        # Category 8: Persistence
        print("üîÑ Category 8: Persistence Indicators...")
        df = self._build_persistence(df)
        
        print(f"\n‚úÖ Built indicators. Final columns: {len(df.columns)}")
        return df
    
    def _build_momentum(self, df):
        """Build momentum indicators."""
        for w in IndicatorConfig.MOMENTUM_WINDOWS:
            col = f'mom_{w}m'
            df[col] = df.groupby('permno')['ret'].transform(
                lambda x: (1 + x).rolling(w).apply(lambda y: y.prod() - 1, raw=True)
            )
        
        # Momentum skipping recent month (classic)
        df['mom_12m_skip1'] = df.groupby('permno')['ret'].transform(
            lambda x: (1 + x.shift(1)).rolling(12).apply(lambda y: y.prod() - 1, raw=True)
        )
        
        # Momentum acceleration
        df['mom_6m_lag'] = df.groupby('permno')['mom_6m'].shift(6)
        df['mom_accel'] = df['mom_6m'] - df['mom_6m_lag']
        
        return df
    
    def _build_reversal(self, df):
        """Build reversal indicators."""
        for w in IndicatorConfig.REVERSAL_WINDOWS:
            col = f'reversal_{w}m'
            df[col] = df.groupby('permno')['ret'].transform(
                lambda x: (1 + x).rolling(w).apply(lambda y: y.prod() - 1, raw=True)
            )
        
        # Mean reversion (distance from rolling mean)
        df['price_ma_12m'] = df.groupby('permno')['abs_prc'].transform(
            lambda x: x.rolling(12).mean()
        )
        df['mean_reversion'] = (df['abs_prc'] - df['price_ma_12m']) / df['price_ma_12m']
        
        # 52-week high/low distance
        df['price_52w_high'] = df.groupby('permno')['abs_prc'].transform(
            lambda x: x.rolling(12).max()
        )
        df['price_52w_low'] = df.groupby('permno')['abs_prc'].transform(
            lambda x: x.rolling(12).min()
        )
        df['dist_52w_high'] = (df['abs_prc'] - df['price_52w_high']) / df['price_52w_high']
        df['dist_52w_low'] = (df['abs_prc'] - df['price_52w_low']) / df['price_52w_low']
        
        return df
    
    def _build_volatility(self, df):
        """Build volatility indicators."""
        # Total volatility (rolling 12-month)
        df['vol_total'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).std()
        )
        
        # Annualized volatility
        df['vol_annual'] = df['vol_total'] * np.sqrt(12)
        
        # Downside volatility (semi-deviation)
        def downside_vol(x):
            neg = x[x < 0]
            return neg.std() if len(neg) > 2 else np.nan
        
        df['vol_downside'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(downside_vol, raw=False)
        )
        
        # Upside volatility
        def upside_vol(x):
            pos = x[x > 0]
            return pos.std() if len(pos) > 2 else np.nan
        
        df['vol_upside'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(upside_vol, raw=False)
        )
        
        # Volatility asymmetry
        df['vol_asymmetry'] = df['vol_upside'] / df['vol_downside'].replace(0, np.nan)
        
        # Merge daily volatility if available
        if self.daily is not None:
            df = df.merge(
                self.daily[['permno', 'yearmonth', 'realized_vol', 'daily_vol']],
                on=['permno', 'yearmonth'],
                how='left'
            )
        
        # Volatility of volatility (meta-risk)
        df['vol_of_vol'] = df.groupby('permno')['vol_total'].transform(
            lambda x: x.rolling(12).std()
        )
        
        return df
    
    def _build_liquidity(self, df):
        """Build liquidity indicators."""
        # Turnover already computed in pull
        # Rolling average turnover
        df['turnover_avg'] = df.groupby('permno')['turnover'].transform(
            lambda x: x.rolling(12).mean()
        )
        
        # Merge daily liquidity metrics if available
        if self.daily is not None:
            if 'amihud' not in df.columns:
                df = df.merge(
                    self.daily[['permno', 'yearmonth', 'amihud', 'zero_day_pct']],
                    on=['permno', 'yearmonth'],
                    how='left'
                )
        
        # Dollar volume (log)
        df['log_dollar_volume'] = np.log1p(df['dollar_volume'])
        
        # Market cap (log)
        df['log_mktcap'] = np.log1p(df['mktcap'])
        
        return df
    
    def _build_factor_loadings(self, df):
        """Build factor loading indicators (beta, etc.)."""
        if self.ff is None:
            print("   ‚ö†Ô∏è No FF factors, skipping factor loadings")
            return df
        
        # Merge FF factors
        df = df.merge(self.ff[['yearmonth', 'mktrf', 'smb', 'hml', 'rf', 'umd']], 
                      on='yearmonth', how='left')
        
        # Excess return
        df['ret_excess'] = df['ret'] - df['rf']
        
        # Rolling CAPM beta (36-month)
        def rolling_beta(group):
            result = pd.Series(index=group.index, dtype=float)
            for i in range(35, len(group)):
                window = group.iloc[i-35:i+1]
                if len(window.dropna()) >= 24:
                    try:
                        cov = window['ret_excess'].cov(window['mktrf'])
                        var = window['mktrf'].var()
                        if var > 0:
                            result.iloc[i] = cov / var
                    except:
                        pass
            return result
        
        print("      Computing rolling betas (this may take a moment)...")
        df['beta_capm'] = df.groupby('permno').apply(
            lambda x: rolling_beta(x)
        ).reset_index(level=0, drop=True)
        
        # Alpha from CAPM
        df['alpha_capm'] = df['ret_excess'] - df['beta_capm'] * df['mktrf']
        
        # Simple approximations for factor loadings (full rolling would be slow)
        # These are placeholder - can be enhanced with true rolling regression
        df['beta_smb'] = df.groupby('permno').apply(
            lambda x: x['ret_excess'].rolling(36).cov(x['smb']) / x['smb'].rolling(36).var()
        ).reset_index(level=0, drop=True) if 'smb' in df.columns else np.nan
        
        df['beta_hml'] = df.groupby('permno').apply(
            lambda x: x['ret_excess'].rolling(36).cov(x['hml']) / x['hml'].rolling(36).var()
        ).reset_index(level=0, drop=True) if 'hml' in df.columns else np.nan
        
        # R-squared with market (proxy)
        df['r_squared_market'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(36).corr(df.loc[x.index, 'mktrf']) ** 2
        )
        
        return df
    
    def _build_risk_adjusted(self, df):
        """Build risk-adjusted performance metrics."""
        # Sharpe ratio (rolling)
        df['sharpe'] = df.groupby('permno').apply(
            lambda x: x['ret'].rolling(12).mean() / x['ret'].rolling(12).std()
        ).reset_index(level=0, drop=True)
        
        # Sortino ratio
        df['sortino'] = df['ret'] / df['vol_downside'].replace(0, np.nan)
        
        # Rolling Sharpe (annual)
        df['sharpe_annual'] = df['sharpe'] * np.sqrt(12)
        
        # Information ratio (alpha / tracking error)
        if 'alpha_capm' in df.columns:
            df['tracking_error'] = df.groupby('permno')['alpha_capm'].transform(
                lambda x: x.rolling(12).std()
            )
            df['information_ratio'] = df['alpha_capm'] / df['tracking_error'].replace(0, np.nan)
        
        # Treynor ratio
        if 'beta_capm' in df.columns:
            df['treynor'] = df['ret'] / df['beta_capm'].replace(0, np.nan)
        
        return df
    
    def _build_tail_risk(self, df):
        """Build tail risk indicators."""
        # VaR (parametric)
        df['var_95'] = df.groupby('permno').apply(
            lambda x: x['ret'].rolling(12).mean() - 1.645 * x['ret'].rolling(12).std()
        ).reset_index(level=0, drop=True)
        
        df['var_99'] = df.groupby('permno').apply(
            lambda x: x['ret'].rolling(12).mean() - 2.326 * x['ret'].rolling(12).std()
        ).reset_index(level=0, drop=True)
        
        # CVaR / Expected Shortfall (5th percentile average)
        def cvar_95(x):
            if len(x) < 12:
                return np.nan
            cutoff = x.quantile(0.05)
            tail = x[x <= cutoff]
            return tail.mean() if len(tail) > 0 else np.nan
        
        df['cvar_95'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(cvar_95, raw=False)
        )
        
        # Skewness
        df['skewness'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).skew()
        )
        
        # Kurtosis
        df['kurtosis'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).kurt()
        )
        
        # Extreme loss/gain probability
        def extreme_loss_prob(x):
            if len(x) < 12:
                return np.nan
            threshold = x.quantile(0.05)
            return (x <= threshold).mean()
        
        def extreme_gain_prob(x):
            if len(x) < 12:
                return np.nan
            threshold = x.quantile(0.95)
            return (x >= threshold).mean()
        
        df['extreme_loss_prob'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(24).apply(extreme_loss_prob, raw=False)
        )
        df['extreme_gain_prob'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(24).apply(extreme_gain_prob, raw=False)
        )
        
        # Gain/Loss ratio
        df['gain_loss_ratio'] = df['extreme_gain_prob'] / df['extreme_loss_prob'].replace(0, np.nan)
        
        # Max drawdown (rolling 12-month)
        def max_drawdown(x):
            if len(x) < 3:
                return np.nan
            cumret = (1 + x).cumprod()
            running_max = cumret.cummax()
            drawdown = (cumret - running_max) / running_max
            return drawdown.min()
        
        df['max_drawdown'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(max_drawdown, raw=False)
        )
        
        return df
    
    def _build_persistence(self, df):
        """Build persistence indicators."""
        # Return autocorrelation
        df['ret_autocorr'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(lambda y: pd.Series(y).autocorr(), raw=False)
        )
        
        # Volatility persistence
        df['vol_persistence'] = df.groupby('permno')['vol_total'].transform(
            lambda x: x.rolling(12).apply(lambda y: pd.Series(y).autocorr(), raw=False)
        )
        
        # Momentum persistence
        df['mom_persistence'] = df.groupby('permno')['mom_6m'].transform(
            lambda x: x.rolling(12).apply(lambda y: pd.Series(y).autocorr(), raw=False)
        )
        
        # Positive month percentage
        df['positive_month_pct'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(12).apply(lambda y: (y > 0).mean(), raw=False)
        )
        
        # Win/Loss streaks (simplified)
        def max_consecutive(x, positive=True):
            if len(x) < 3:
                return np.nan
            binary = (x > 0) if positive else (x < 0)
            max_streak = 0
            current = 0
            for b in binary:
                if b:
                    current += 1
                    max_streak = max(max_streak, current)
                else:
                    current = 0
            return max_streak
        
        df['streak_max_win'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(24).apply(lambda y: max_consecutive(y, True), raw=False)
        )
        df['streak_max_loss'] = df.groupby('permno')['ret'].transform(
            lambda x: x.rolling(24).apply(lambda y: max_consecutive(y, False), raw=False)
        )
        
        return df


# ================================================================
# DIAGNOSTIC SUITE
# ================================================================
class DiagnosticSuite:
    """Comprehensive CEO-Match diagnostics for all indicators."""
    
    def __init__(self, indicator_df, match_df, link_df):
        """
        Args:
            indicator_df: DataFrame with all stock indicators
            match_df: DataFrame with CEO-firm match quality
            link_df: CRSP-Compustat link table
        """
        self.indicators = indicator_df
        self.match = match_df
        self.link = link_df
        self.merged = None
        
        # Indicator columns (will be populated)
        self.indicator_cols = []
        
    def merge_data(self):
        """Merge indicator data with match quality."""
        print("\n" + "=" * 70)
        print("üîó MERGING INDICATORS WITH CEO-MATCH DATA")
        print("=" * 70)
        
        # Add gvkey to indicators via link
        ind = self.indicators.merge(
            self.link[['permno', 'gvkey']].drop_duplicates(),
            on='permno',
            how='left'
        )
        
        # Aggregate to annual (take year-end values)
        ind['year'] = ind['date'].dt.year
        ind_annual = ind.groupby(['gvkey', 'year']).last().reset_index()
        
        # Merge with match data
        self.merged = self.match.merge(
            ind_annual,
            left_on=['gvkey', 'fiscalyear'],
            right_on=['gvkey', 'year'],
            how='left'
        )
        
        # Identify indicator columns
        base_cols = ['gvkey', 'year', 'fiscalyear', 'permno', 'date', 'match_means', 
                     'match_quintile', 'ret', 'prc', 'vol', 'shrout']
        self.indicator_cols = [c for c in ind_annual.columns 
                               if c not in base_cols and ind_annual[c].dtype in ['float64', 'float32', 'int64']]
        
        # Create match quintiles
        self.merged['match_q'] = pd.qcut(
            self.merged['match_means'], 5, 
            labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'], 
            duplicates='drop'
        )
        
        print(f"   Merged dataset: {len(self.merged):,} observations")
        print(f"   Indicator columns: {len(self.indicator_cols)}")
        print(f"   Match coverage: {self.merged['match_means'].notna().sum():,}")
        
        return self.merged
    
    def run_stratification(self):
        """Run stratification analysis for all indicators."""
        print("\n" + "=" * 70)
        print("üìä A. STRATIFICATION ANALYSIS")
        print("=" * 70)
        
        results = []
        
        for col in self.indicator_cols:
            df = self.merged.dropna(subset=[col, 'match_q'])
            if len(df) < 100:
                continue
            
            # Compute quintile means
            quintile_means = df.groupby('match_q', observed=False)[col].mean()
            
            # Q5-Q1 spread
            try:
                q5 = quintile_means.loc['Q5']
                q1 = quintile_means.loc['Q1']
                spread = q5 - q1
                
                # T-test
                q1_vals = df[df['match_q'] == 'Q1'][col].dropna()
                q5_vals = df[df['match_q'] == 'Q5'][col].dropna()
                if len(q1_vals) > 10 and len(q5_vals) > 10:
                    t_stat, p_val = stats.ttest_ind(q5_vals, q1_vals)
                else:
                    t_stat, p_val = np.nan, np.nan
                
                # Monotonicity check
                is_monotonic = quintile_means.is_monotonic_increasing or quintile_means.is_monotonic_decreasing
                
                results.append({
                    'indicator': col,
                    'Q1': q1,
                    'Q2': quintile_means.loc['Q2'],
                    'Q3': quintile_means.loc['Q3'],
                    'Q4': quintile_means.loc['Q4'],
                    'Q5': q5,
                    'Q5_Q1_spread': spread,
                    't_stat': t_stat,
                    'p_value': p_val,
                    'monotonic': is_monotonic,
                    'n_obs': len(df)
                })
            except:
                continue
        
        result_df = pd.DataFrame(results)
        
        # Sort by absolute t-stat
        result_df['abs_t'] = result_df['t_stat'].abs()
        result_df = result_df.sort_values('abs_t', ascending=False)
        
        # Print top findings
        print("\n--- TOP 20 INDICATORS BY MATCH RELATIONSHIP ---")
        print(result_df[['indicator', 'Q1', 'Q5', 'Q5_Q1_spread', 't_stat', 'p_value', 'monotonic']].head(20).to_string())
        
        # Count significant
        sig_pos = (result_df['t_stat'] > 1.96).sum()
        sig_neg = (result_df['t_stat'] < -1.96).sum()
        print(f"\n   üìä Significant positive (t>1.96): {sig_pos}")
        print(f"   üìä Significant negative (t<-1.96): {sig_neg}")
        
        return result_df
    
    def run_regressions(self):
        """Run regression analysis for key indicators."""
        print("\n" + "=" * 70)
        print("üìà B. REGRESSION ANALYSIS")
        print("=" * 70)
        
        # Select key indicators for regression
        key_indicators = [
            'sharpe', 'vol_total', 'beta_capm', 'alpha_capm',
            'mom_12m', 'max_drawdown', 'skewness', 'gain_loss_ratio',
            'turnover', 'ret_autocorr'
        ]
        
        results = []
        
        for ind in key_indicators:
            if ind not in self.merged.columns:
                continue
            
            reg_df = self.merged.dropna(subset=[ind, 'match_means', 'logatw'])
            if len(reg_df) < 200:
                continue
            
            # Univariate
            try:
                m1 = ols(f'{ind} ~ match_means', data=reg_df).fit()
                
                # With controls
                m2 = ols(f'{ind} ~ match_means + logatw', data=reg_df).fit()
                
                # With more controls
                control_cols = ['logatw', 'exp_roa', 'rdintw', 'leverage']
                available_controls = [c for c in control_cols if c in reg_df.columns and reg_df[c].notna().sum() > 100]
                if available_controls:
                    formula = f'{ind} ~ match_means + ' + ' + '.join(available_controls)
                    m3 = ols(formula, data=reg_df.dropna(subset=available_controls)).fit()
                else:
                    m3 = m2
                
                results.append({
                    'indicator': ind,
                    'beta_univar': m1.params['match_means'],
                    't_univar': m1.tvalues['match_means'],
                    'r2_univar': m1.rsquared,
                    'beta_ctrl': m3.params['match_means'],
                    't_ctrl': m3.tvalues['match_means'],
                    'r2_ctrl': m3.rsquared,
                    'n_obs': len(reg_df)
                })
            except Exception as e:
                print(f"   Error with {ind}: {e}")
                continue
        
        result_df = pd.DataFrame(results)
        
        print("\n--- REGRESSION RESULTS ---")
        print(result_df.round(4).to_string())
        
        return result_df
    
    def run_ml_analysis(self):
        """Run machine learning feature importance analysis."""
        print("\n" + "=" * 70)
        print("ü§ñ C. MACHINE LEARNING ANALYSIS")
        print("=" * 70)
        
        # Prepare data
        feature_cols = [c for c in self.indicator_cols if c in self.merged.columns]
        feature_cols = feature_cols[:30]  # Limit for speed
        
        ml_df = self.merged[['match_means'] + feature_cols].dropna()
        
        if len(ml_df) < 500:
            print("   ‚ö†Ô∏è Insufficient data for ML analysis")
            return None
        
        X = ml_df[feature_cols]
        y = ml_df['match_means']
        
        # Random Forest
        print("\n   Training Random Forest...")
        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
        rf.fit(X, y)
        
        # Feature importance
        importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\n--- RANDOM FOREST FEATURE IMPORTANCE ---")
        print(importance.head(15).to_string())
        
        # Gradient Boosting (for comparison)
        print("\n   Training Gradient Boosting...")
        gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
        gb.fit(X, y)
        
        importance_gb = pd.DataFrame({
            'feature': feature_cols,
            'importance_gb': gb.feature_importances_
        }).sort_values('importance_gb', ascending=False)
        
        print("\n--- GRADIENT BOOSTING FEATURE IMPORTANCE ---")
        print(importance_gb.head(15).to_string())
        
        return importance.merge(importance_gb, on='feature')
    
    def run_interaction_analysis(self):
        """Analyze Match √ó Size and Match √ó Industry interactions."""
        print("\n" + "=" * 70)
        print("üîÄ D. INTERACTION ANALYSIS")
        print("=" * 70)
        
        # Key indicators for interaction
        key_ind = ['sharpe', 'vol_total', 'beta_capm', 'mom_12m']
        available_ind = [k for k in key_ind if k in self.merged.columns]
        
        if not available_ind:
            print("   ‚ö†Ô∏è No key indicators available")
            return None
        
        # Size √ó Match interaction
        print("\n--- SIZE √ó MATCH INTERACTION ---")
        size_df = self.merged.dropna(subset=['logatw', 'match_q'])
        size_df['size_q'] = pd.qcut(size_df['logatw'], 4, labels=['Small', 'Mid', 'Large', 'Giant'])
        
        for ind in available_ind:
            if ind not in size_df.columns:
                continue
            ind_df = size_df.dropna(subset=[ind])
            if len(ind_df) < 100:
                continue
            
            heatmap = ind_df.groupby(['match_q', 'size_q'], observed=False)[ind].mean().unstack()
            print(f"\n   {ind}:")
            print(heatmap.round(4).to_string())
        
        # Industry √ó Match interaction
        if 'compindustry' in self.merged.columns:
            print("\n--- INDUSTRY √ó MATCH INTERACTION (Top 5 Industries) ---")
            top_industries = self.merged['compindustry'].value_counts().head(5).index
            ind_df = self.merged[self.merged['compindustry'].isin(top_industries)]
            
            for ind in available_ind[:2]:
                if ind not in ind_df.columns:
                    continue
                temp = ind_df.dropna(subset=[ind])
                if len(temp) < 50:
                    continue
                heatmap = temp.groupby(['match_q', 'compindustry'], observed=False)[ind].mean().unstack()
                print(f"\n   {ind}:")
                print(heatmap.round(4).to_string())
        
        return True
    
    def run_persistence_analysis(self):
        """Analyze whether match quality predicts future indicator stability."""
        print("\n" + "=" * 70)
        print("üîÑ E. PERSISTENCE ANALYSIS")
        print("=" * 70)
        
        # Sort by firm-year
        df = self.merged.sort_values(['gvkey', 'fiscalyear'])
        
        # Key indicators
        key_ind = ['sharpe', 'vol_total', 'ret']
        available_ind = [k for k in key_ind if k in df.columns]
        
        results = []
        
        for ind in available_ind:
            # Lead indicator (next year)
            df[f'{ind}_lead'] = df.groupby('gvkey')[ind].shift(-1)
            
            # Regression: Future indicator ~ Current Match + Current Indicator
            reg_df = df.dropna(subset=[f'{ind}_lead', 'match_means', ind])
            
            if len(reg_df) < 200:
                continue
            
            try:
                m = ols(f'{ind}_lead ~ match_means + {ind}', data=reg_df).fit()
                results.append({
                    'indicator': ind,
                    'match_beta': m.params['match_means'],
                    'match_t': m.tvalues['match_means'],
                    'current_beta': m.params[ind],
                    'current_t': m.tvalues[ind],
                    'r2': m.rsquared,
                    'n_obs': len(reg_df)
                })
            except:
                continue
        
        if results:
            result_df = pd.DataFrame(results)
            print("\n--- PERSISTENCE REGRESSIONS ---")
            print("   (Future indicator ~ Match + Current indicator)")
            print(result_df.round(4).to_string())
            return result_df
        
        return None
    
    def create_visualizations(self):
        """Create comprehensive visualization dashboard."""
        print("\n" + "=" * 70)
        print("üìä CREATING MEGA VISUALIZATION DASHBOARD")
        print("=" * 70)
        
        fig = plt.figure(figsize=(32, 28))
        
        # Get available indicators
        key_indicators = {
            'sharpe': '‚öñÔ∏è Sharpe Ratio',
            'vol_total': 'üìâ Total Volatility',
            'beta_capm': '‚ö° CAPM Beta',
            'alpha_capm': 'üìà CAPM Alpha',
            'mom_12m': 'üöÄ 12M Momentum',
            'max_drawdown': 'üìâ Max Drawdown',
            'skewness': 'üìä Skewness',
            'kurtosis': 'üìä Kurtosis',
            'gain_loss_ratio': 'üé≤ Gain/Loss Ratio',
            'turnover': 'üíß Turnover',
            'var_95': '‚ö†Ô∏è VaR 95%',
            'cvar_95': '‚ö†Ô∏è CVaR 95%',
            'ret_autocorr': 'üîÑ Return Autocorr',
            'vol_persistence': 'üîÑ Vol Persistence',
            'positive_month_pct': '‚úÖ % Positive Months',
            'information_ratio': 'üìä Information Ratio'
        }
        
        available = {k: v for k, v in key_indicators.items() if k in self.merged.columns}
        
        plot_idx = 1
        num_plots = min(len(available), 20)
        
        for ind, title in list(available.items())[:num_plots]:
            ax = fig.add_subplot(5, 4, plot_idx)
            
            data = self.merged.dropna(subset=[ind, 'match_q'])
            if len(data) < 50:
                plot_idx += 1
                continue
            
            means = data.groupby('match_q', observed=False)[ind].mean()
            colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#27ae60']
            ax.bar(range(5), means.values, color=colors, edgecolor='black')
            ax.set_xticks(range(5))
            ax.set_xticklabels(['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])
            ax.set_title(title, fontweight='bold', fontsize=10)
            ax.set_ylabel(ind, fontsize=8)
            
            plot_idx += 1
        
        plt.tight_layout()
        plt.savefig(f'{IndicatorConfig.OUTPUT_DIR}/stock_indicator_suite.png', dpi=150, bbox_inches='tight')
        print(f"   Saved: {IndicatorConfig.OUTPUT_DIR}/stock_indicator_suite.png")
        
        # Create correlation heatmap
        self._create_correlation_heatmap()
        
    def _create_correlation_heatmap(self):
        """Create indicator correlation heatmap."""
        print("\n   Creating correlation heatmap...")
        
        key_cols = ['sharpe', 'vol_total', 'beta_capm', 'mom_12m', 'max_drawdown',
                    'skewness', 'turnover', 'var_95', 'ret_autocorr', 'match_means']
        available = [c for c in key_cols if c in self.merged.columns]
        
        if len(available) < 4:
            return
        
        corr_matrix = self.merged[available].corr()
        
        fig, ax = plt.subplots(figsize=(12, 10))
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', 
                    center=0, ax=ax, square=True)
        ax.set_title('üî¨ Indicator Correlation Matrix', fontweight='bold', fontsize=14)
        
        plt.tight_layout()
        plt.savefig(f'{IndicatorConfig.OUTPUT_DIR}/indicator_correlation.png', dpi=150, bbox_inches='tight')
        print(f"   Saved: {IndicatorConfig.OUTPUT_DIR}/indicator_correlation.png")


# ================================================================
# MAIN EXECUTION
# ================================================================
def main():
    print("üî¨" * 40)
    print("   ULTIMATE STOCK INDICATOR SUITE & CEO-MATCH DIAGNOSTICS")
    print("üî¨" * 40)
    
    config = Config()
    
    # ================================================================
    # PHASE 1: LOAD BASE DATA & TRAIN MODEL
    # ================================================================
    print("\nüìÇ LOADING BASE MATCH DATA...")
    processor = DataProcessor(config)
    raw_df = processor.load_data()
    df_clean = processor.prepare_features(raw_df)
    print(f"  Match Quality Data: {len(df_clean):,} obs")
    
    print("\nüß† TRAINING TWO-TOWER MODEL...")
    train_df, val_df = train_test_split(df_clean, test_size=0.2, random_state=42)
    processor.fit(train_df)
    train_data = processor.transform(train_df)
    val_data = processor.transform(val_df)
    train_loader = DataLoader(CEOFirmDataset(train_data), batch_size=config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(CEOFirmDataset(val_data), batch_size=config.BATCH_SIZE, shuffle=False)
    model = train_model(train_loader, val_loader, train_data, config)
    
    df_clean['match_quintile'] = pd.qcut(df_clean['match_means'], 5, 
        labels=['Q1 (Worst)', 'Q2', 'Q3', 'Q4', 'Q5 (Best)'])
    
    # ================================================================
    # PHASE 2: WRDS DATA PULL
    # ================================================================
    print("\nüîå CONNECTING TO WRDS...")
    db = connect_wrds()
    
    if db is None:
        print("‚ùå Could not connect to WRDS. Using cached data if available.")
        # Try to load cached data
        try:
            crsp_monthly = pd.read_parquet(f'{IndicatorConfig.OUTPUT_DIR}/crsp_monthly_full.parquet')
            crsp_daily = pd.read_parquet(f'{IndicatorConfig.OUTPUT_DIR}/crsp_daily_agg.parquet')
            ff_factors = pd.read_parquet(f'{IndicatorConfig.OUTPUT_DIR}/ff_factors.parquet')
            link_df = pd.read_parquet(f'{IndicatorConfig.OUTPUT_DIR}/wrds_crsp_comp.parquet')
            print("‚úÖ Loaded cached CRSP data")
        except:
            print("‚ùå No cached data available. Exiting.")
            return
    else:
        # Pull fresh data
        crsp_monthly = pull_crsp_monthly(db)
        crsp_daily = pull_crsp_daily(db)
        ff_factors = pull_ff_factors(db)
        link_df = pull_link_table(db)
        
        db.close()
        print("\n‚úÖ WRDS Connection Closed")
        
        # Save to cache
        if crsp_monthly is not None:
            crsp_monthly.to_parquet(f'{IndicatorConfig.OUTPUT_DIR}/crsp_monthly_full.parquet')
        if crsp_daily is not None:
            crsp_daily.to_parquet(f'{IndicatorConfig.OUTPUT_DIR}/crsp_daily_agg.parquet')
        if ff_factors is not None:
            ff_factors.to_parquet(f'{IndicatorConfig.OUTPUT_DIR}/ff_factors.parquet')
    
    # ================================================================
    # PHASE 3: BUILD INDICATORS
    # ================================================================
    builder = IndicatorBuilder(crsp_monthly, crsp_daily, ff_factors)
    indicator_df = builder.build_all()
    
    # Save indicators
    indicator_df.to_parquet(f'{IndicatorConfig.OUTPUT_DIR}/stock_indicators_full.parquet')
    print(f"\n   Saved: {IndicatorConfig.OUTPUT_DIR}/stock_indicators_full.parquet")
    
    # ================================================================
    # PHASE 4: RUN DIAGNOSTICS
    # ================================================================
    diagnostics = DiagnosticSuite(indicator_df, df_clean, link_df)
    merged = diagnostics.merge_data()
    
    # A. Stratification
    strat_results = diagnostics.run_stratification()
    
    # B. Regressions
    reg_results = diagnostics.run_regressions()
    
    # C. Machine Learning
    ml_results = diagnostics.run_ml_analysis()
    
    # D. Interactions
    diagnostics.run_interaction_analysis()
    
    # E. Persistence
    persist_results = diagnostics.run_persistence_analysis()
    
    # ================================================================
    # PHASE 5: VISUALIZATION
    # ================================================================
    diagnostics.create_visualizations()
    
    # ================================================================
    # EXECUTIVE SUMMARY
    # ================================================================
    print("\n" + "=" * 70)
    print("üèÜ EXECUTIVE SUMMARY")
    print("=" * 70)
    
    # Count key findings
    n_indicators = len(diagnostics.indicator_cols)
    n_significant = len(strat_results[strat_results['p_value'] < 0.05]) if strat_results is not None else 0
    n_monotonic = strat_results['monotonic'].sum() if strat_results is not None else 0
    
    # Get top findings
    if strat_results is not None and len(strat_results) > 0:
        top_pos = strat_results[strat_results['t_stat'] > 0].head(3)
        top_neg = strat_results[strat_results['t_stat'] < 0].head(3)
    
    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                 üî¨üî¨üî¨ STOCK INDICATOR SUITE FINDINGS üî¨üî¨üî¨                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                                    ‚ïë
‚ïë  üìä INDICATOR COVERAGE                                                            ‚ïë
‚ïë     Total Indicators Built: {n_indicators:,}                                               ‚ïë
‚ïë     Statistically Significant (p<0.05): {n_significant:,}                                  ‚ïë
‚ïë     Monotonic Relationships: {n_monotonic:,}                                               ‚ïë
‚ïë                                                                                    ‚ïë
‚ïë  üìà TOP POSITIVE MATCH RELATIONSHIPS                                              ‚ïë
‚ïë     (Higher match quality ‚Üí Higher indicator value)                               ‚ïë
""")
    
    if strat_results is not None:
        for _, row in top_pos.iterrows():
            print(f"‚ïë     {row['indicator']}: t={row['t_stat']:.2f}, spread={row['Q5_Q1_spread']:.4f}")
    
    print("""‚ïë                                                                                    ‚ïë
‚ïë  üìâ TOP NEGATIVE MATCH RELATIONSHIPS                                              ‚ïë
‚ïë     (Higher match quality ‚Üí Lower indicator value)                                ‚ïë
""")
    
    if strat_results is not None:
        for _, row in top_neg.iterrows():
            print(f"‚ïë     {row['indicator']}: t={row['t_stat']:.2f}, spread={row['Q5_Q1_spread']:.4f}")
    
    print("""‚ïë                                                                                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Save results
    if strat_results is not None:
        strat_results.to_csv(f'{IndicatorConfig.OUTPUT_DIR}/stratification_results.csv', index=False)
    if reg_results is not None:
        reg_results.to_csv(f'{IndicatorConfig.OUTPUT_DIR}/regression_results.csv', index=False)
    if ml_results is not None:
        ml_results.to_csv(f'{IndicatorConfig.OUTPUT_DIR}/ml_importance.csv', index=False)
    
    # Save merged dataset
    merged.to_parquet(f'{IndicatorConfig.OUTPUT_DIR}/ceo_match_indicators_merged.parquet')
    
    print("\nüî¨üî¨üî¨ STOCK INDICATOR SUITE COMPLETE! üî¨üî¨üî¨")


if __name__ == "__main__":
    main()
